{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35887\n",
      "2304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = pd.read_csv('../Downloads/fer2013/fer2013.csv')\n",
    "print(len(emotions))\n",
    "print(len(emotions.loc[0, 'pixels'].split(' ')))\n",
    "emotions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Usage</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrivateTest</th>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PublicTest</th>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>28709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             emotion\n",
       "Usage               \n",
       "PrivateTest     3589\n",
       "PublicTest      3589\n",
       "Training       28709"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training-Public_Test-Private_Test split\n",
    "emotions.groupby('Usage').count()[['emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage</th>\n",
       "      <th>Pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4953</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>547</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5121</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8989</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6077</td>\n",
       "      <td>16.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4002</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6198</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Usage   Pct\n",
       "emotion             \n",
       "0         4953  13.8\n",
       "1          547   1.5\n",
       "2         5121  14.3\n",
       "3         8989  25.0\n",
       "4         6077  16.9\n",
       "5         4002  11.2\n",
       "6         6198  17.3"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency of each label\n",
    "print('0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral')\n",
    "table = emotions.groupby('emotion').count()[['Usage']]\n",
    "table['Pct'] = table['Usage']/table['Usage'].sum()\n",
    "table['Pct'] = table['Pct'].map(lambda x: round(x, 3)*100)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Input into Tensors\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   70    80    82  ...    106   109    82\n",
       "  151   150   147  ...    193   183   184\n",
       "  231   212   156  ...     88   110   152\n",
       "   24    32    36  ...    142   143   142\n",
       "    4     0     0  ...     30    29    30\n",
       "[torch.FloatTensor of size 5x2304]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Process Input into Tensors\n",
    "pixel_matrix = np.zeros(35887*2304).reshape(35887, 2304)\n",
    "for r in range(len(emotions)):\n",
    "    pixel_matrix[r, :] = np.array(emotions.loc[r, 'pixels'].split(' ')).astype(float)\n",
    "\n",
    "Pixel_Tensor = torch.from_numpy(pixel_matrix).float()\n",
    "Emotion_Tensor = torch.from_numpy(np.asarray(emotions['emotion']))\n",
    "Pixel_Tensor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = Pixel_Tensor[:28709]\n",
    "X_test = Pixel_Tensor[28709:]\n",
    "\n",
    "y_train = Emotion_Tensor[:28709]\n",
    "y_test = Emotion_Tensor[28709:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   70    80    82  ...    106   109    82\n",
       "  151   150   147  ...    193   183   184\n",
       "  231   212   156  ...     88   110   152\n",
       "   24    32    36  ...    142   143   142\n",
       "    4     0     0  ...     30    29    30\n",
       "[torch.FloatTensor of size 5x2304]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model - Feedforward Neutral Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network1Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(Network1Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layer1 = nn.Linear(input_size, n_hidden)\n",
    "        self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.layer3 = nn.Linear(n_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return F.log_softmax(x) \n",
    "        #use log_softmax to convert vector elements to have probabilistic interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(batch_size, sequences, labels):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = sequences.size()[0]\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "            break\n",
    "    \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch_indices_tensor = torch.LongTensor(batch_indices)\n",
    "        batch_train = sequences[batch_indices_tensor].type(torch.FloatTensor)\n",
    "        batch_train_labels = labels[batch_indices_tensor]\n",
    "        yield [batch_train, batch_train_labels]\n",
    "\n",
    "def eval_iter(batch_size,sequence_tensors,labels):\n",
    "    '''Returns list of length batch_size, each entry is a\n",
    "    tuple with LongTensors of sequences and labels, respectively'''\n",
    "    batches = []\n",
    "    dataset_size = len(sequence_tensors)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch_indices_tensor = torch.LongTensor(batch_indices)\n",
    "        batch_sequences = sequence_tensors[batch_indices_tensor].type(torch.FloatTensor)\n",
    "        batch_test_labels = labels[batch_indices_tensor]\n",
    "        if len(batch_sequences) == batch_size:\n",
    "            batches.append((torch.stack(batch_sequences), batch_test_labels))\n",
    "        else:\n",
    "            continue\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Loop\n",
    "\n",
    "* Loop batches of samples in the training set\n",
    "* Run each batch through the model (forward pass)\n",
    "* Compute the loss\n",
    "* Compute the gradients with respect to model parameters (backward pass)\n",
    "* Update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Motivation behine model.train vs. model.eval\n",
    "#Sometimes, there are techniques such as Dropout (to avoid overfitting) that you only want to use \n",
    "#during training, not testing\n",
    "\n",
    "def train(epoch, train_iter):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_iter):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(X_train),\n",
    "                100 * batch_idx * len(data) / len(X_train), loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Testing Loop\n",
    "\n",
    "* Loop over batches of samples in the testing set\n",
    "* Run each batch through the model (forward pass)\n",
    "* Compute the loss and accuracy\n",
    "* Do not compute gradients or update model parameters \n",
    "* We are saving the testing data to evaluate how the model is doing on data it has not been trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similar to training loop, except you're not altering the parameters.\n",
    "\n",
    "def test(test_iter):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_iter):\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(X_test)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(X_test),\n",
    "        100 * correct / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(num_epochs, train_iter, test_iter):\n",
    "    epoch = 0\n",
    "    while epoch < num_epochs:\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_iter):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(X_train),\n",
    "                    100 * batch_idx * len(data) / len(X_train), loss.data[0]))\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_iter):\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss                                                               \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(X_test)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(X_test),\n",
    "            100 * correct / len(X_test)))\n",
    "        \n",
    "        epoch=epoch+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training settings \n",
    "input_size  = 48*48   # images are 48x48 pixels\n",
    "n_hidden    = 100     # number of hidden units\n",
    "output_size = 7      # there are 7 classes - seven different types of emotions\n",
    "\n",
    "model = Network1Layer(input_size, n_hidden, output_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "batch_size = 50\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "* We will only train for a few epochs here\n",
    "* Normally we would train for longer\n",
    "* Depending on the dataset and model size, this can take days or weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/28709 (0%)]\tLoss: 8.705260\n",
      "Train Epoch: 1 [5000/28709 (17%)]\tLoss: 1.868483\n",
      "Train Epoch: 1 [10000/28709 (35%)]\tLoss: 2.717513\n",
      "Train Epoch: 1 [15000/28709 (52%)]\tLoss: 1.887941\n",
      "Train Epoch: 1 [20000/28709 (70%)]\tLoss: 1.846126\n",
      "Train Epoch: 1 [25000/28709 (87%)]\tLoss: 1.845099\n",
      "\n",
      "Test set: Average loss: 1.9141, Accuracy: 1770/7178 (25%)\n",
      "\n",
      "Train Epoch: 2 [0/28709 (0%)]\tLoss: 1.799254\n",
      "Train Epoch: 2 [5000/28709 (17%)]\tLoss: 1.786282\n",
      "Train Epoch: 2 [10000/28709 (35%)]\tLoss: 1.819339\n",
      "Train Epoch: 2 [15000/28709 (52%)]\tLoss: 1.827134\n",
      "Train Epoch: 2 [20000/28709 (70%)]\tLoss: 1.755584\n",
      "Train Epoch: 2 [25000/28709 (87%)]\tLoss: 1.727374\n",
      "\n",
      "Test set: Average loss: 1.8988, Accuracy: 1770/7178 (25%)\n",
      "\n",
      "Train Epoch: 3 [0/28709 (0%)]\tLoss: 1.772365\n",
      "Train Epoch: 3 [5000/28709 (17%)]\tLoss: 1.803323\n",
      "Train Epoch: 3 [10000/28709 (35%)]\tLoss: 1.797836\n",
      "Train Epoch: 3 [15000/28709 (52%)]\tLoss: 1.759488\n",
      "Train Epoch: 3 [20000/28709 (70%)]\tLoss: 1.776778\n",
      "Train Epoch: 3 [25000/28709 (87%)]\tLoss: 1.841123\n",
      "\n",
      "Test set: Average loss: 1.8972, Accuracy: 1770/7178 (25%)\n",
      "\n",
      "Train Epoch: 4 [0/28709 (0%)]\tLoss: 1.806855\n",
      "Train Epoch: 4 [5000/28709 (17%)]\tLoss: 1.841559\n",
      "Train Epoch: 4 [10000/28709 (35%)]\tLoss: 1.711345\n",
      "Train Epoch: 4 [15000/28709 (52%)]\tLoss: 1.805961\n",
      "Train Epoch: 4 [20000/28709 (70%)]\tLoss: 1.884523\n",
      "Train Epoch: 4 [25000/28709 (87%)]\tLoss: 1.823748\n",
      "\n",
      "Test set: Average loss: 1.8919, Accuracy: 1769/7178 (25%)\n",
      "\n",
      "Train Epoch: 5 [0/28709 (0%)]\tLoss: 1.768186\n",
      "Train Epoch: 5 [5000/28709 (17%)]\tLoss: 1.741693\n",
      "Train Epoch: 5 [10000/28709 (35%)]\tLoss: 1.747062\n",
      "Train Epoch: 5 [15000/28709 (52%)]\tLoss: 1.824804\n",
      "Train Epoch: 5 [20000/28709 (70%)]\tLoss: 1.758098\n",
      "Train Epoch: 5 [25000/28709 (87%)]\tLoss: 1.740860\n",
      "\n",
      "Test set: Average loss: 1.8916, Accuracy: 1762/7178 (25%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_iter = batch_iter(batch_size, X_train, y_train)\n",
    "#test_iter = eval_iter(batch_size, X_test, y_test)\n",
    "\n",
    "#train_test(num_epochs, train_iter, test_iter)\n",
    "#Don't use accuracy as the loss, because we want the loss function to be differentiable!\n",
    "for epoch in range(1, 6):\n",
    "    train_iter = batch_iter(batch_size, X_train, y_train)\n",
    "    test_iter = eval_iter(batch_size, X_test, y_test)\n",
    "    train(epoch, train_iter)\n",
    "    test(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
